\documentclass[a4paper,11pt]{article}
\usepackage[osf]{mathpazo}
\usepackage{ms}
\usepackage[]{natbib}
\raggedright

\newcommand{\datastorr}{\texttt{datastorr}}
\newcommand{\smurl}[1]{{\footnotesize\url{#1}}}
\usepackage{graphicx}

\title{{\datastorr}: A package for distributing dynamic and versioned
data}
\author{Richard G. FitzJohn$^1$, Daniel S. Falster$^2$, Matthew
  W. Pennell$^3$, and William K. Cornwells$^4$}
\affiliation{
$^1$ Somewhere in the UK\\
$^2$ Everywhere else\\
$^3$ Department of Zoology and Biodiversity Research Centre,
University of British Columbia, Vancouver, B.C. V6T 1Z4 Canada\\

}
\date{}

\bibliographystyle{mee}

\usepackage[title,titletoc,toc]{appendix}

\mstype{Applications Note}
\runninghead{The {\datastorr} package}
\keywords{}

\begin{document}
\mstitlepage
\noindent
\parindent=1.5em
\addtolength{\parskip}{.3em}
\doublespacing
\linenumbers
\section{Summary}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\itemsep1pt\parskip0pt\parsep0pt
\item Data is cool
\end{enumerate}

\subsection{Introduction}

Data is quickly becoming a first-class scientific product. Funding bodies, publishers, and scientific social norms are increasingly both requiring data to be published alongside research publications and recognizing the value of producing and sharing data for its own sake. This has been a rapid, and exciting, sea change and we as a community are still catching up and trying to discern how best to disseminate data and credit data creators and aggregators. One thing that is clear is that there is not a one-size-fits-all solution to this problem as data is incredibly heterogeneous. On one end of the spectrum are massive, centralized repositories that are constantly being added to; this is a great solution to storing and distributing relatively standardized data such as sequences (e.g., \texttt{GenBank}, \texttt{SRA}). On the other end are repositories for storing a wide variety of static, stand-alone datasets (\texttt{Dryad}, \texttt{Figshare}, and less optimally, as supplements to research papers. The static model works very well for reproducing a \emph{particular} analysis in a \emph{particular} paper, which is crucial and important for reproducibility. While both of these classes of data are well-served by current infrastructure, we argue that there is a middle ground that does not fit either of these paradigms very well.

Consider the case of the data from an ecological meta-analysis. It is only a snapshot of our existing knowledge at a given time. When a new study is published the static dataset from the last meta-analysis immediately becomes obsolete --- to keep up-to-date that new study should be added. However, if we have a static data distribution system, this is impossible. Another problem with the static model is the discovery of small errors. For any much medium-sized database that is repeatedly used, errors will be found, and there needs to be some organized way to correct them. And while a centralized GenBank-esque repository would be useful for updating or adding to the data from this meta-analysis, the infrastructure investment (designing maintaining, and hosting the database) is clearly overkill for most projects and is a very substantial barrier to entry.

\subsection{Versioning data}

Almost all useful databases are living--they \emph{should} get better
through time either by the addition of new data or by improving the
quality of existing data. For example a dataset on plant characteristics
might become obsolete as taxonomic changes occur. This might only
require a small amount of work and a minor change, but if there is no
way to distribute that new dataset, then that work has to be re-done by
every user of that dataset. Moreover, those required changes adds steps
between the canonical dataset and the new analysis (and publication),
making reproducibility of the new analysis more difficult than it needs
to be. If the canonical dataset is versioned--and versions are
accessible--the up-to-date data can be distributed, used, and analyses
based on these data can be reproduced.

\subsection{One solution for small-to-medium sized
datasets}

Versioned data is laudable goal, but implementation is difficult.

For finding the right tool to version data, the size of the dataset is
an important consideration. ``Big data'' requires a separate set of
hardware and software tools (see Amazon web services); examples of big
data include genetic datasets, global datasets species observations, and
remote sensing data. This size of this data is bigger than individual
research groups and often requires a specialized governmental or
non-governmental institution with full-time staff to curate and maintain
those research resources.

We were particularly interested in smaller and more specialized
problems--the type of datasets that a given research group might
specialize in. These are the types of research questions that lend
themselves to periodic analysis (or meta-analyses): - does chocolate
consumption correlate with obesity? - does the rate of spread of
epidemics correlate with transmission rates? - how do plants partition
resources in different conditions? - does the body size of animals
correlate with range size? - which birds have been observed in an annual
survey in a given place?

{[}\textbf{MWP:} I think the focus on questions is perhaps misplaced. We
should instead focus on the qualities of the data that we are interested
in.{]}

Each of those questions might be the captured in a dataset that could be
one lab's or NGO's expertise--or more commonly a compendium of work by
many different groups, harvested from the original researchers or from
the literature. In many cases, the key data for these questions will not
be ``big data''--they'll be small to possibly medium sized. And the
questions may drive a given research line, but are not in most cases
general enough to establish infra-structure for. But crucially the data
themselves will change over time. And currently there is no
infrastructure for organizing and distributing these types of ``living
datasets''.

So what we need in this case is a low-cost (both money and time) way of
making versions of a dataset and delivering it to potential users, along
with notes on what has changed and why since the previous version.
Ideally periodically versions can be deposited and given a digital
object identifier (DOI) at a major research archive, but in between
major versions, small changes can be made, errors can be corrected, and
the most recent version can be distributed to users. Idealy there would
also be a way for users to easily flag errors for the currator to then
correct in the next release. As many of the users of small-to-medium
sized datasets use the open source language R, we also prioritized easy
of use from within R.

For small datasets, it is possible to use the same tools developed for
big data: there are a number of high-setup-cost options (stand-alone
websites, APIs), which have advantages if you have the resources to
develop and maintain them. We wanted a relatively low cost way (both in
money and time).

\subsection{Our system}

Our approach builds on the theory of ``semantic versioning'' developed
for software distribution; this system uses the familiar 1.0.0 notation.
We argue that data versioning is a similar problem, and we are not the
first to make this argument (INSERT LINKS HERE). Applying semantic
versioning to data, a change from 1.0.0 to 1.0.1 implies a tiny changes,
for example a minor error correction. A change from 1.0.0 to 1.1.0
implies a major change, for example adding a new study to a meta-analysis
dataset. A change from 1.0.0 to 2.0.0 implies a very major revision, for
example improving the entire structure of the dataset and adding new
columns.

One of the key problems in switching from a static to a dynamic data
world, is that it's currently hard to set up and maintain a system like
this. Ideally using a versioned database should be easy for both the
user and the data maintainer. This is especially crucial is the data
maintainers are busy. Moreover, old versions need to be preserved in a
stable and accessible way.

To try and achieve to above goals, we built a system to distribute
versions of our datasets. We think that with this infrastructure, it is
relatively easy to set up and maintain a versioned system with any
small-to-medium sized dataset.

In our system, the data is downloaded only once per computer, available
for offline access, easy to port to other platforms, and can get a doi
when desired (i.e.~when it's used for a publication). All old versions
are accessible.

\subsection{Some worked examples}

So far we have three:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \texttt{Biomass And Allometry Database} \citep{Falster-2015}
\item
  \texttt{taxonlookup} \citep{Pennell-2015a}
\item
  \texttt{plantgrowthform}
\end{enumerate}

Each of these is a versioned dataset, designed for a particular research
goal, and also explicitly flexible such that they can grow in the
future. In all three cases, the user needs to install a few R packages.
The data versions themselves are stored as Github Releases (as well as
if desired on DataDryad or FigShare), then any of the versions of the
data can be downloaded and used with only one of \texttt{R} code.

\subsection{How much expertise is needed to use a versioned
dataset?}

Very very basic \texttt{R}. With 4 lines of \texttt{R} code, you can
have the data downloaded and loaded into R and ready for use.

\subsection{How much expertise is needed to set a system like this
up?}

Basically there are two general skills required: 1) understanding
version control via \texttt{git} and 2) writing a very simple R package.
From the prolific Hadley Wickham there is a
\href{http://r-pkgs.had.co.nz/git.html}{very clear introduction to git}
and \href{http://r-pkgs.had.co.nz/intro.html}{an instruction to the
basics of R packages}.

With those two skills the rest should be relatively easy. To set up a
living dataset you need to provide:
\begin{enumerate}
  \item a github repository name (e.g.~wcornwell/taxonlookup, traitecoevo/baad.data).
  \item A single file that will be the versioned dataset (e.g. \texttt{plant\_lookup.csv} or \texttt{baad\_data.zip}).
  \item A function to load that data into R:(e.g.~a simple wrapper around \texttt{read.csv}).
\end{enumerate}

Then as the data changes (hopefully improves) with time, update the data
file and create a github release with a new version number (preferably
using \href{http://semver.org/}{semantic versioning}). All the versions
will be available to the user. And versions can be given a unique DOI
when appropriate, e.g.~when they are used in a publication.

\subsection{Conclusion}

Because it builds on existing infra-structure for distributing software,
this system requires very little (or no) money to set up and only a
small amount of programming expertise. The current infrastructure will
not work for big or continually updated data (e.g.~weather stations or
satellite downloads). For a set of problems that need periodically
updated small-to-medium sized data (e.g.~most meta-analysis problems),
this might be a useful approach.

\bibliography{refs}

\end{document}

